# Mistral Generate Description Service

Text generation service using the Mistral-7B-Instruct-v0.1 model. This service processes text descriptions of images and generates structured product descriptions with titles, descriptions, keywords, and categories.

## üìã Description

The service uses the Mistral-7B-Instruct-v0.1 model, a powerful language model trained to follow instructions and generate high-quality text. The service exposes a REST API that accepts image descriptions and returns structured product information generated by the model.

## üîÑ API

### Main Endpoint: `/api/v1/run`

**Method**: POST

**Request Body**:
```json
{
  "text": "A detailed description of an image",
  "prompt": "Custom instruction prompt" (optional)
}
```

**Successful Response**:
```json
{
  "id": "uuid",
  "status": "COMPLETED",
  "output": {
    "status": "COMPLETED",
    "message": "Product description generated successfully.",
    "data": "{\"title\": \"Product Title\", \"description\": \"Product description...\", \"keywords\": [\"keyword1\", \"keyword2\"], \"category\": \"Category\"}"
  }
}
```

### Status Endpoint: `/api/v1/status/{id}`

**Method**: GET

**Response**:
```json
{
  "id": "uuid",
  "status": "COMPLETED",
  "output": {
    "status": "COMPLETED",
    "message": "Product description generated successfully.",
    "data": "{\"title\": \"Product Title\", \"description\": \"Product description...\", \"keywords\": [\"keyword1\", \"keyword2\"], \"category\": \"Category\"}"
  }
}
```

## üß† Internal Operation

The service follows these steps to process a text description:

1. Prepares a prompt with the input text and system instructions
2. Formats the input for the Mistral model using the chat template
3. Generates text using the model with appropriate parameters
4. Processes the response and returns structured JSON data
5. Handles formatting for compatibility with the main backend

The service uses PyTorch with GPU acceleration for fast inference and is designed to run in environments with CUDA-capable GPUs.

## üè† Development

### Local Setup
Create virtual environment and install dependencies using venv.

```bash
# 1. Create virtual environment
python -m venv venv

# 2. Activate virtual environment
# Windows:
venv\Scripts\activate
# Linux/Mac:
# source venv/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Install PyTorch manually (not included in requirements.txt)
# For CPU-only development:
pip install torch --index-url https://download.pytorch.org/whl/cpu

# For CUDA development (if you have compatible GPU):
pip install torch --index-url https://download.pytorch.org/whl/cu121

# 5. Copy .env.example to .env
cp .env.example .env

# 6. Run service
python main.py
```

### Deactivate environment
```bash
deactivate
```

## üöÄ Production (RunPod)

The Dockerfile is optimized for RunPod deployment:
- Uses PyTorch base image with CUDA support
- Automatic GPU detection
- Volume mounting for model caching
- Optimized for serverless operation
